{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn import preprocessing, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Import Linear Regression and a regularized regression function\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.decomposition import PCA\n",
    "# Finally, import function to make a machine learning pipeline\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data:(1212, 888)\n",
      "Test Data:(776, 888)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>75.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>76.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id     y\n",
       "0  0.0  75.0\n",
       "1  1.0  76.0"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('data/X_train.csv')\n",
    "Y = pd.read_csv('data/Y_train.csv')\n",
    "X_test = pd.read_csv('data/X_test.csv')\n",
    "\n",
    "print(\"Training Data:{}\".format(X.shape))\n",
    "print(\"Test Data:{}\".format(X_test.shape))\n",
    "\n",
    "Y.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1212.000000\n",
       "mean       69.763201\n",
       "std         9.941656\n",
       "min        42.000000\n",
       "25%        64.000000\n",
       "50%        70.000000\n",
       "75%        76.000000\n",
       "max        96.000000\n",
       "Name: y, dtype: float64"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y['y'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>x0</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>x3</th>\n",
       "      <th>x4</th>\n",
       "      <th>x5</th>\n",
       "      <th>x6</th>\n",
       "      <th>x7</th>\n",
       "      <th>x8</th>\n",
       "      <th>...</th>\n",
       "      <th>x877</th>\n",
       "      <th>x878</th>\n",
       "      <th>x879</th>\n",
       "      <th>x880</th>\n",
       "      <th>x881</th>\n",
       "      <th>x882</th>\n",
       "      <th>x883</th>\n",
       "      <th>x884</th>\n",
       "      <th>x885</th>\n",
       "      <th>x886</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1212.000000</td>\n",
       "      <td>1122.000000</td>\n",
       "      <td>1.140000e+03</td>\n",
       "      <td>1.132000e+03</td>\n",
       "      <td>1123.000000</td>\n",
       "      <td>1133.000000</td>\n",
       "      <td>1126.000000</td>\n",
       "      <td>1141.000000</td>\n",
       "      <td>1122.000000</td>\n",
       "      <td>1122.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.127000e+03</td>\n",
       "      <td>1.127000e+03</td>\n",
       "      <td>1.132000e+03</td>\n",
       "      <td>1127.000000</td>\n",
       "      <td>1.125000e+03</td>\n",
       "      <td>1127.000000</td>\n",
       "      <td>1148.000000</td>\n",
       "      <td>1126.000000</td>\n",
       "      <td>1123.000000</td>\n",
       "      <td>1111.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>605.500000</td>\n",
       "      <td>7300.504957</td>\n",
       "      <td>1.003125e+06</td>\n",
       "      <td>1.051614e+06</td>\n",
       "      <td>1049.844772</td>\n",
       "      <td>105.047681</td>\n",
       "      <td>203511.156265</td>\n",
       "      <td>1050.735880</td>\n",
       "      <td>341958.172245</td>\n",
       "      <td>104916.372111</td>\n",
       "      <td>...</td>\n",
       "      <td>3.825322e+11</td>\n",
       "      <td>1.003145e+06</td>\n",
       "      <td>-5.025956e+05</td>\n",
       "      <td>1001.891614</td>\n",
       "      <td>9.995905e+05</td>\n",
       "      <td>3732.365716</td>\n",
       "      <td>100.659348</td>\n",
       "      <td>1617.956555</td>\n",
       "      <td>10505.966555</td>\n",
       "      <td>65052.578569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>350.018571</td>\n",
       "      <td>1379.891266</td>\n",
       "      <td>1.001817e+05</td>\n",
       "      <td>2.818085e+04</td>\n",
       "      <td>28.475255</td>\n",
       "      <td>2.823009</td>\n",
       "      <td>29841.633207</td>\n",
       "      <td>28.623527</td>\n",
       "      <td>58820.438523</td>\n",
       "      <td>2755.013692</td>\n",
       "      <td>...</td>\n",
       "      <td>4.347959e+11</td>\n",
       "      <td>9.594847e+04</td>\n",
       "      <td>8.608874e+04</td>\n",
       "      <td>100.410174</td>\n",
       "      <td>9.680491e+04</td>\n",
       "      <td>725.532171</td>\n",
       "      <td>9.336065</td>\n",
       "      <td>401.791865</td>\n",
       "      <td>290.648021</td>\n",
       "      <td>0.029221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1030.502715</td>\n",
       "      <td>6.716345e+05</td>\n",
       "      <td>1.000037e+06</td>\n",
       "      <td>1000.062471</td>\n",
       "      <td>100.033879</td>\n",
       "      <td>63202.600024</td>\n",
       "      <td>1000.134779</td>\n",
       "      <td>92365.078214</td>\n",
       "      <td>100016.602565</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.083882e+11</td>\n",
       "      <td>7.186635e+05</td>\n",
       "      <td>-1.110029e+06</td>\n",
       "      <td>643.042857</td>\n",
       "      <td>6.895354e+05</td>\n",
       "      <td>451.131089</td>\n",
       "      <td>65.692019</td>\n",
       "      <td>458.289896</td>\n",
       "      <td>10001.346875</td>\n",
       "      <td>65052.528022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>302.750000</td>\n",
       "      <td>6496.988432</td>\n",
       "      <td>9.409699e+05</td>\n",
       "      <td>1.028118e+06</td>\n",
       "      <td>1025.913567</td>\n",
       "      <td>102.724769</td>\n",
       "      <td>186609.583069</td>\n",
       "      <td>1026.464126</td>\n",
       "      <td>309182.739540</td>\n",
       "      <td>102687.100342</td>\n",
       "      <td>...</td>\n",
       "      <td>1.578603e+11</td>\n",
       "      <td>9.385117e+05</td>\n",
       "      <td>-5.475966e+05</td>\n",
       "      <td>933.591537</td>\n",
       "      <td>9.348641e+05</td>\n",
       "      <td>3297.203036</td>\n",
       "      <td>94.515998</td>\n",
       "      <td>1360.553119</td>\n",
       "      <td>10249.981685</td>\n",
       "      <td>65052.553207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>605.500000</td>\n",
       "      <td>7381.752216</td>\n",
       "      <td>1.003238e+06</td>\n",
       "      <td>1.052406e+06</td>\n",
       "      <td>1050.174694</td>\n",
       "      <td>105.023063</td>\n",
       "      <td>201709.971057</td>\n",
       "      <td>1051.399190</td>\n",
       "      <td>337308.178918</td>\n",
       "      <td>104861.600927</td>\n",
       "      <td>...</td>\n",
       "      <td>2.758197e+11</td>\n",
       "      <td>1.001974e+06</td>\n",
       "      <td>-4.965350e+05</td>\n",
       "      <td>1001.295903</td>\n",
       "      <td>9.989761e+05</td>\n",
       "      <td>3768.931107</td>\n",
       "      <td>100.672131</td>\n",
       "      <td>1604.528424</td>\n",
       "      <td>10505.538263</td>\n",
       "      <td>65052.579678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>908.250000</td>\n",
       "      <td>8153.767104</td>\n",
       "      <td>1.070372e+06</td>\n",
       "      <td>1.075329e+06</td>\n",
       "      <td>1074.864998</td>\n",
       "      <td>107.391464</td>\n",
       "      <td>220981.402036</td>\n",
       "      <td>1075.166305</td>\n",
       "      <td>371797.754187</td>\n",
       "      <td>107160.482832</td>\n",
       "      <td>...</td>\n",
       "      <td>4.867297e+11</td>\n",
       "      <td>1.063238e+06</td>\n",
       "      <td>-4.560057e+05</td>\n",
       "      <td>1069.927335</td>\n",
       "      <td>1.064618e+06</td>\n",
       "      <td>4179.602230</td>\n",
       "      <td>106.809319</td>\n",
       "      <td>1861.784028</td>\n",
       "      <td>10763.810688</td>\n",
       "      <td>65052.603107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1211.000000</td>\n",
       "      <td>13055.814408</td>\n",
       "      <td>1.316548e+06</td>\n",
       "      <td>1.099990e+06</td>\n",
       "      <td>1099.845375</td>\n",
       "      <td>110.048177</td>\n",
       "      <td>370398.522988</td>\n",
       "      <td>1099.997865</td>\n",
       "      <td>784817.830992</td>\n",
       "      <td>109991.914244</td>\n",
       "      <td>...</td>\n",
       "      <td>7.405700e+12</td>\n",
       "      <td>1.308895e+06</td>\n",
       "      <td>-1.400403e+05</td>\n",
       "      <td>1323.073354</td>\n",
       "      <td>1.276136e+06</td>\n",
       "      <td>6781.164024</td>\n",
       "      <td>126.678078</td>\n",
       "      <td>3745.022165</td>\n",
       "      <td>10999.908941</td>\n",
       "      <td>65052.627907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 888 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id            x0            x1            x2           x3  \\\n",
       "count  1212.000000   1122.000000  1.140000e+03  1.132000e+03  1123.000000   \n",
       "mean    605.500000   7300.504957  1.003125e+06  1.051614e+06  1049.844772   \n",
       "std     350.018571   1379.891266  1.001817e+05  2.818085e+04    28.475255   \n",
       "min       0.000000   1030.502715  6.716345e+05  1.000037e+06  1000.062471   \n",
       "25%     302.750000   6496.988432  9.409699e+05  1.028118e+06  1025.913567   \n",
       "50%     605.500000   7381.752216  1.003238e+06  1.052406e+06  1050.174694   \n",
       "75%     908.250000   8153.767104  1.070372e+06  1.075329e+06  1074.864998   \n",
       "max    1211.000000  13055.814408  1.316548e+06  1.099990e+06  1099.845375   \n",
       "\n",
       "                x4             x5           x6             x7             x8  \\\n",
       "count  1133.000000    1126.000000  1141.000000    1122.000000    1122.000000   \n",
       "mean    105.047681  203511.156265  1050.735880  341958.172245  104916.372111   \n",
       "std       2.823009   29841.633207    28.623527   58820.438523    2755.013692   \n",
       "min     100.033879   63202.600024  1000.134779   92365.078214  100016.602565   \n",
       "25%     102.724769  186609.583069  1026.464126  309182.739540  102687.100342   \n",
       "50%     105.023063  201709.971057  1051.399190  337308.178918  104861.600927   \n",
       "75%     107.391464  220981.402036  1075.166305  371797.754187  107160.482832   \n",
       "max     110.048177  370398.522988  1099.997865  784817.830992  109991.914244   \n",
       "\n",
       "           ...               x877          x878          x879         x880  \\\n",
       "count      ...       1.127000e+03  1.127000e+03  1.132000e+03  1127.000000   \n",
       "mean       ...       3.825322e+11  1.003145e+06 -5.025956e+05  1001.891614   \n",
       "std        ...       4.347959e+11  9.594847e+04  8.608874e+04   100.410174   \n",
       "min        ...      -5.083882e+11  7.186635e+05 -1.110029e+06   643.042857   \n",
       "25%        ...       1.578603e+11  9.385117e+05 -5.475966e+05   933.591537   \n",
       "50%        ...       2.758197e+11  1.001974e+06 -4.965350e+05  1001.295903   \n",
       "75%        ...       4.867297e+11  1.063238e+06 -4.560057e+05  1069.927335   \n",
       "max        ...       7.405700e+12  1.308895e+06 -1.400403e+05  1323.073354   \n",
       "\n",
       "               x881         x882         x883         x884          x885  \\\n",
       "count  1.125000e+03  1127.000000  1148.000000  1126.000000   1123.000000   \n",
       "mean   9.995905e+05  3732.365716   100.659348  1617.956555  10505.966555   \n",
       "std    9.680491e+04   725.532171     9.336065   401.791865    290.648021   \n",
       "min    6.895354e+05   451.131089    65.692019   458.289896  10001.346875   \n",
       "25%    9.348641e+05  3297.203036    94.515998  1360.553119  10249.981685   \n",
       "50%    9.989761e+05  3768.931107   100.672131  1604.528424  10505.538263   \n",
       "75%    1.064618e+06  4179.602230   106.809319  1861.784028  10763.810688   \n",
       "max    1.276136e+06  6781.164024   126.678078  3745.022165  10999.908941   \n",
       "\n",
       "               x886  \n",
       "count   1111.000000  \n",
       "mean   65052.578569  \n",
       "std        0.029221  \n",
       "min    65052.528022  \n",
       "25%    65052.553207  \n",
       "50%    65052.579678  \n",
       "75%    65052.603107  \n",
       "max    65052.627907  \n",
       "\n",
       "[8 rows x 888 columns]"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace missing values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing entries in training data across all features:  77121\n",
      "Missing entries in test data across all features:  46382\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing entries in training data across all features: \",X.isnull().sum().sum())\n",
    "X[X.x0.isnull()].head(5)\n",
    "\n",
    "print(\"Missing entries in test data across all features: \",X_test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "X = X.interpolate(method = 'nearest').ffill().bfill()\n",
    "X_test = X_test.interpolate(method = 'nearest').ffill().bfill()\n",
    "print(X.isnull().sum().sum())\n",
    "print(X_test.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get column names first\n",
    "#names = X.columns\n",
    "# Create the Scaler object\n",
    "#scaler = preprocessing.StandardScaler()\n",
    "# Fit your data on the scaler object\n",
    "#X_normalized = scaler.fit_transform(X)\n",
    "#X_test_normalized = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_normalized = preprocessing.normalize(X)\n",
    "X_test_normalized = preprocessing.normalize(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training / Validation Spilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (969, 888)\n",
      "X_val: (243, 888)\n"
     ]
    }
   ],
   "source": [
    "# Using normalized X here!\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_normalized, Y, test_size=0.2, random_state=42)\n",
    "print(\"X_train: {}\".format(X_train.shape))\n",
    "print(\"X_val: {}\".format(X_val.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove features with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape of X_train:(969, 888)\n",
      "initial shape of X_val:(243, 888)\n",
      "initial shape of X_test:(776, 888)\n",
      "(888,)\n",
      "new shape of X_train:(969, 884)\n",
      "new shape of X_val:(243, 884)\n",
      "new shape of X_test:(776, 884)\n"
     ]
    }
   ],
   "source": [
    "print(\"initial shape of X_train:{}\".format(X_train.shape))\n",
    "print(\"initial shape of X_val:{}\".format(X_val.shape))\n",
    "print(\"initial shape of X_test:{}\".format(X_test_normalized.shape))\n",
    "\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(X_train)\n",
    "\n",
    "X_train = selector.transform(X_train)\n",
    "indices_retained = selector.get_support(indices=False)\n",
    "print(indices_retained.shape)\n",
    "# need to convert X_val to pandas dataframe to use .loc to select features\n",
    "X_val_df = (pd.DataFrame(data=X_val))\n",
    "X_val = X_val_df.loc[:,indices_retained]\n",
    "X_test_normalized_df = pd.DataFrame(data=X_test_normalized)\n",
    "X_test_normalized = X_test_normalized_df.loc[:,indices_retained]\n",
    "\n",
    "print(\"new shape of X_train:{}\".format(X_train.shape))\n",
    "print(\"new shape of X_val:{}\".format(X_val.shape))\n",
    "print(\"new shape of X_test:{}\".format(X_test_normalized.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove features which highly correlate with other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial shape of X_train:(969, 884)\n",
      "initial shape of X_val:(243, 884)\n",
      "initial shape of X_test:(776, 884)\n",
      "new shape of X_train:(969, 13)\n",
      "new shape of X_val:(243, 13)\n",
      "new shape of X_test:(776, 13)\n"
     ]
    }
   ],
   "source": [
    "print(\"initial shape of X_train:{}\".format(X_train.shape))\n",
    "print(\"initial shape of X_val:{}\".format(X_val.shape))\n",
    "print(\"initial shape of X_test:{}\".format(X_test_normalized.shape))\n",
    "\n",
    "# Create correlation matrix\n",
    "X_train_df = (pd.DataFrame(data=X_train))\n",
    "corr_matrix = X_train_df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.8\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]\n",
    "\n",
    "# Drop features \n",
    "X_train = X_train_df.drop(X_train_df.columns[to_drop], axis=1)\n",
    "X_val = X_val.drop(X_val.columns[to_drop], axis=1)\n",
    "X_test_normalized = X_test_normalized.drop(X_test_normalized.columns[to_drop], axis=1)\n",
    "\n",
    "print(\"new shape of X_train:{}\".format(X_train.shape))\n",
    "print(\"new shape of X_val:{}\".format(X_val.shape))\n",
    "print(\"new shape of X_test:{}\".format(X_test_normalized.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a153ac2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_columns = X_train.shape[1]\n",
    "print(n_columns)\n",
    "pca = PCA(n_components = n_columns)\n",
    "pca.fit(X_train)\n",
    "components = np.linspace(1, n_columns, n_columns)\n",
    "explained_var = np.zeros(n_columns)\n",
    "explained_var[0] = pca.explained_variance_[0]\n",
    "for i in range(1,n_columns):\n",
    "    explained_var[i-1] = explained_var[i-2] + pca.explained_variance_[i-1]\n",
    "\n",
    "plt.plot(components,explained_var, label=\"Sum of explained variance\")\n",
    "plt.xlabel('number of PCA components')\n",
    "plt.ylabel('explained variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score: 0.48\n"
     ]
    }
   ],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "regr.fit(X_train, Y_train)\n",
    "# Make predictions using the validation set\n",
    "Y_pred = regr.predict(X_val)\n",
    "\n",
    "print('R2 score: %.2f' % r2_score(Y_val, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alpha (regularization strength) of LASSO regression\n",
    "lasso_eps = 0.0001\n",
    "lasso_nalpha=20\n",
    "lasso_iter=5000\n",
    "# Min and max degree of polynomials features to consider\n",
    "degree_min = 2\n",
    "degree_max = 8\n",
    "# Test/train split\n",
    "# Make a pipeline model with polynomial transformation and LASSO regression with cross-validation, run it for increasing degree of polynomial (complexity of the model)\n",
    "# for degree in range(degree_min,degree_max+1):\n",
    "#     model = make_pipeline(PolynomialFeatures(degree, interaction_only=False), LassoCV(eps=lasso_eps,n_alphas=lasso_nalpha,max_iter=lasso_iter,\n",
    "# normalize=True,cv=5))\n",
    "#     model.fit(X_train,Y_train)\n",
    "#     Y_pred = np.array(model.predict(X_test))\n",
    "    \n",
    "#     print('R2 score: %.2f' % r2_score(Y_test, Y_pred))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
